
autoencoder:
  target: ldm.models.autoencoder.VQModelTorch
  ckpt_path: weights/autoencoder_vq_f4.pth
  use_fp16: True
  tune_decoder: False
  params:
    lora_tune_decoder: False
    embed_dim: 3
    n_embed: 8192
    ddconfig:
      double_z: False
      z_channels: 3
      resolution: 256
      in_channels: 3
      out_ch: 3
      ch: 128
      ch_mult:
      - 1
      - 2
      - 4
      num_res_blocks: 2
      attn_resolutions: []
      dropout: 0.0
      padding_mode: zeros

model:
  target: models.unet.UNetModelSwin
  ckpt_path: weights/resshift_realsrx4_s4_v3.pth
  params:
    image_size: 64
    in_channels: 3
    model_channels: 160
    out_channels: ${autoencoder.params.embed_dim}
    attention_resolutions: [64,32,16,8]
    dropout: 0
    channel_mult: [1, 2, 2, 4]
    num_res_blocks: [2, 2, 2, 2]
    conv_resample: True
    dims: 2
    use_fp16: False
    num_head_channels: 32
    use_scale_shift_norm: True
    resblock_updown: False
    swin_depth: 2
    swin_embed_dim: 192
    window_size: 8
    mlp_ratio: 4
    cond_lq: True
    lq_size: 64

diffusion:
  target: models.script_util.create_gaussian_diffusion
  params:
    sf: 4
    schedule_name: exponential
    schedule_kwargs:
      power: 0.3
    etas_end: 0.99
    steps: 4
    min_noise_level: 0.2
    kappa: 2.0
    weighted_mse: False
    predict_type: xstart
    timestep_respacing: ~
    scale_factor: 1.0
    normalize_input: True
    latent_flag: True



data:
  train:
    params:
      dir_paths: ['C:\training_dataset\div2k\DIV2K_train_HR\DIV2K_train_HR']
      gt_size: 256
  val:
    params:
      dir_paths: ['C:\training_dataset\div2k\DIV2K_train_HR\DIV2K_train_HR']


train:
  lr: 5e-5                      # learning rate
  batch: [32, 4]
  microbatch: 16
  num_workers: 16
  epochs: 100000
  log_freq: [1, 20, 100]         # [training loss, val images(epochs), saving .pth(epochs)]

